/**
 * @fileoverview Freesail Agent — MCP Client with LangChain + Gemini
 *
 * This agent connects to the Freesail gateway via MCP (Model Context Protocol)
 * and uses LangChain with Gemini to process chat messages and create dynamic UIs.
 *
 * The MCP server provides:
 * - System prompt (a2ui_system) with protocol docs and catalog info
 * - UI tools (create_surface, update_components, update_data_model, delete_surface)
 * - Action tools (get_pending_actions, get_all_pending_actions)
 * - Session tools (list_sessions)
 */

import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from '@langchain/core/messages';
import { tool, type DynamicStructuredTool } from '@langchain/core/tools';
import { z } from 'zod';
import { LangChainAdapter } from '@freesail/agentruntime';
import type { Client } from '@modelcontextprotocol/sdk/client/index.js';
import { logger } from '@freesail/logger';

/**
 * Agent configuration.
 */
export interface AgentConfig {
  /** Google API Key for Gemini */
  googleApiKey: string;
  /** MCP Client connected to the Freesail gateway */
  mcpClient: Client;
}

/**
 * Chat message format.
 */
export interface ChatMessage {
  role: 'user' | 'assistant';
  content: string;
}



// ============================================================================
// Agent
// ============================================================================

const FALLBACK_SYSTEM_PROMPT = `You are a helpful AI assistant that can create visual UI components using the available tools.

When the user asks to show something visually, use the tools to create UI surfaces with components.
Always respond conversationally AND create UI when appropriate.`;

/**
 * Create the Freesail agent powered by MCP + LangChain + Gemini.
 *
 * The agent:
 * 1. Fetches its system prompt from the MCP server (includes protocol docs + catalogs)
 * 2. Discovers available tools from MCP and wraps them for LangChain
 * 3. Routes all tool calls through the MCP client to the gateway
 */
export function createAgent(config: AgentConfig) {
  const { googleApiKey, mcpClient } = config;

  // Initialize Gemini model
  const model = new ChatGoogleGenerativeAI({
    apiKey: googleApiKey,
    model: 'gemini-2.5-flash',
    temperature: 0.7,
  });

  // Per-session conversation history
  const sessionHistories = new Map<string, (HumanMessage | AIMessage | ToolMessage)[]>();

  function getHistory(sessionId: string): (HumanMessage | AIMessage | ToolMessage)[] {
    if (!sessionHistories.has(sessionId)) {
      sessionHistories.set(sessionId, []);
    }
    return sessionHistories.get(sessionId)!;
  }

  // Cached MCP data
  let cachedSystemPrompt: string | null = null;
  let cachedTools: DynamicStructuredTool[] | null = null;

  /**
   * Fetch the system prompt from the MCP server.
   * The prompt is dynamically generated with current catalog information.
   */
  async function getSystemPrompt(): Promise<string> {
    if (cachedSystemPrompt) return cachedSystemPrompt;

    try {
      const result = await mcpClient.getPrompt({ name: 'a2ui_system' });
      cachedSystemPrompt = result.messages
        .map((m: any) => {
          if (typeof m.content === 'string') return m.content;
          if (m.content.type === 'text') return m.content.text;
          return '';
        })
        .join('\n');
      console.log('[Agent] System prompt loaded from MCP server');
    } catch (error) {
      console.error('[Agent] Failed to get MCP prompt, using fallback:', error);
      cachedSystemPrompt = FALLBACK_SYSTEM_PROMPT;
    }

    return cachedSystemPrompt!;
  }



  /**
   * Build LangChain tools from MCP tools.
   */
  async function getTools(): Promise<DynamicStructuredTool[]> {
    if (cachedTools) return cachedTools;

    const tools = await LangChainAdapter.getTools(mcpClient);
    cachedTools = tools;
    
    console.log(
      `[Agent] Loaded ${tools.length} tools from MCP:`,
      tools.map((t: any) => t.name).join(', ')
    );

    return tools;
  }

  /**
   * Invalidate cached prompt and tools.
   * Called when MCP resources/prompts change (e.g., new catalogs registered).
   */
  function invalidateCache(): void {
    cachedSystemPrompt = null;
    cachedTools = null;
  }

  /**
   * Process a chat message and return the response.
   * Uses the MCP-provided system prompt and tools.
   */
  async function chat(
    userMessage: string,
    sessionId: string = 'default',
    processActionParams?: { 
      onToken?: (token: string) => void; 
    }
  ): Promise<string> {
    const systemPrompt = await getSystemPrompt();
    const currentTools = await getTools();
    const modelWithTools = model.bindTools(currentTools);

    const conversationHistory = getHistory(sessionId);

    conversationHistory.push(new HumanMessage(userMessage));

    const messages = [
      new SystemMessage(systemPrompt),
      ...conversationHistory,
    ];

    // Call model with tools using stream
    let responseChunk = await streamModelResponse(modelWithTools, messages, processActionParams?.onToken);

    // Track tool messages for this turn (kept separate until the turn ends)
    const turnToolMessages: (AIMessage | ToolMessage)[] = [];

    // Process tool calls iteratively
    while (responseChunk.tool_calls && responseChunk.tool_calls.length > 0) {
      // Add AI message with tool calls to turn history
      turnToolMessages.push(new AIMessage({
        content: typeof responseChunk.content === 'string' ? responseChunk.content : '',
        tool_calls: responseChunk.tool_calls,
      }));

      // Execute each tool call
      const toolMessages: ToolMessage[] = [];
      for (const toolCall of responseChunk.tool_calls) {
        const matchedTool = currentTools.find(t => t.name === toolCall.name);
        let result: string;

        try {
          if (matchedTool) {
            result = String(await matchedTool.invoke(toolCall.args));
          } else {
            result = `Unknown tool: ${toolCall.name}`;
          }
        } catch (error) {
          result = `Error: ${error instanceof Error ? error.message : String(error)}`;
          console.error(`[Agent] Tool error (${toolCall.name}):`, error);
        }

        toolMessages.push(new ToolMessage({
          content: result,
          name: toolCall.name,
          tool_call_id: toolCall.id ?? toolCall.name,
        }));
      }

      turnToolMessages.push(...toolMessages);

      responseChunk = await streamModelResponse(
        modelWithTools,
        [
          new SystemMessage(systemPrompt),
          ...conversationHistory,
          ...turnToolMessages,
        ],
        processActionParams?.onToken
      );
    }

    // Conversation ended — persist all tool interactions so the LLM
    // remembers what tools it called and what values it set (e.g. which
    // data model values were the original ones).
    conversationHistory.push(...turnToolMessages);

    // After all tools are done, there may be a final text response.
    const assistantMessage = typeof responseChunk.content === 'string'
      ? responseChunk.content
      : Array.isArray(responseChunk.content)
        ? responseChunk.content.map((p: any) => p.text ?? '').join('')
        : JSON.stringify(responseChunk.content);

    console.log(`[Agent] Final assistant message parsed, length: ${assistantMessage?.length}, raw content:`, JSON.stringify(responseChunk.content));

    if (assistantMessage && assistantMessage.trim() !== '') {
      conversationHistory.push(new AIMessage(assistantMessage));
    } else {
      // If the final response is empty (e.g. after silent tool calls), log it but don't add an empty AI message to history.
      logger.info('[Agent] Final response chunk was empty (likely a quiet tool execution sequence).');
    }

    return assistantMessage;
  }

  /**
   * Helper to stream from the model and call onToken, while returning the final composed message chunk.
   */
  async function streamModelResponse(
    modelWithTools: any, 
    messages: any[], 
    onToken?: (token: string) => void
  ): Promise<any> {
    const stream = await modelWithTools.stream(messages);
    let finalChunk: any | null = null;
    let accumulatedContent = '';

    for await (const chunk of stream) {
      if (typeof chunk.content === 'string' && chunk.content) {
        // Send the new slice of text
        if (onToken) onToken(chunk.content);
        accumulatedContent += chunk.content;
      } else if (Array.isArray(chunk.content)) {
        for (const part of chunk.content) {
          if (part.type === 'text' && part.text) {
             if (onToken) onToken(part.text);
             accumulatedContent += part.text;
          }
        }
      }

      if (!finalChunk) {
        finalChunk = chunk;
      } else {
        finalChunk = finalChunk.concat(chunk);
      }
    }
    
    console.log(`[Agent] streamModelResponse finished. Content type: ${typeof finalChunk?.content}, isArray: ${Array.isArray(finalChunk?.content)}, text length: ${accumulatedContent.length}`);
    
    // Fallback logic for Gemini 2.5 Flash which sometimes embeds function calls inside 'content'
    if (finalChunk && (!finalChunk.tool_calls || finalChunk.tool_calls.length === 0) && Array.isArray(finalChunk.content)) {
      const extractedToolCalls = [];
      for (const part of finalChunk.content) {
        if (part.functionCall) {
           extractedToolCalls.push({
             name: part.functionCall.name,
             args: part.functionCall.args,
             id: `call_${Math.random().toString(36).substring(2, 9)}`,
           });
        }
      }
      if (extractedToolCalls.length > 0) {
        finalChunk.tool_calls = extractedToolCalls;
        // Clean out the content array to just be text parts, or an empty string, so the LLM format doesn't break
        const textParts = finalChunk.content.filter((p: any) => p.type === 'text');
        finalChunk.content = textParts.length > 0 ? textParts : '';
      }
    }

    if (finalChunk && finalChunk.tool_calls) {
      console.log(`[Agent] streamModelResponse yielded ${finalChunk.tool_calls.length} tool calls.`);
    }
    
    return finalChunk;
  }

  /**
   * Clear conversation history for a session, or all sessions.
   */
  function clearHistory(sessionId?: string): void {
    if (sessionId) {
      sessionHistories.delete(sessionId);
    } else {
      sessionHistories.clear();
    }
  }

  return { chat, clearHistory, invalidateCache };
}
